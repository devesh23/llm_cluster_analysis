{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM-Based Semantic Cluster Analysis\n",
    "\n",
    "This notebook performs cluster analysis on semantic data using Azure AI Foundry APIs.\n",
    "\n",
    "## Features\n",
    "- Handles multi-line semantic data grouped by `sequence_uuid`\n",
    "- **Three clustering methods**: Traditional (K-means, DBSCAN) and LLM-based clustering\n",
    "- **LLM Clustering**: Uses AI to intelligently group data based on semantic meaning\n",
    "- Uses Azure AI embeddings for semantic understanding\n",
    "- Automatically generates cluster titles using LLM\n",
    "- Visualizes clusters and their characteristics\n",
    "\n",
    "## Prerequisites\n",
    "Before running this notebook, ensure you have:\n",
    "1. Azure AI Foundry API credentials\n",
    "2. Set environment variables:\n",
    "   - `AZURE_AI_ENDPOINT`: Your Azure AI endpoint URL\n",
    "   - `AZURE_AI_KEY`: Your Azure AI API key\n",
    "   - `AZURE_AI_MODEL_NAME`: The model name for embeddings (e.g., 'text-embedding-ada-002')\n",
    "   - `AZURE_AI_CHAT_MODEL`: The chat model for cluster naming (e.g., 'gpt-4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Azure AI\n",
    "from azure.ai.inference import ChatCompletionsClient, EmbeddingsClient\n",
    "from azure.ai.inference.models import SystemMessage, UserMessage\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "# Clustering and ML\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Utilities\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"\u2713 Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Azure AI Configuration\n",
    "AZURE_AI_ENDPOINT = os.getenv('AZURE_AI_ENDPOINT', '')\n",
    "AZURE_AI_KEY = os.getenv('AZURE_AI_KEY', '')\n",
    "AZURE_AI_MODEL_NAME = os.getenv('AZURE_AI_MODEL_NAME', 'text-embedding-ada-002')\n",
    "AZURE_AI_CHAT_MODEL = os.getenv('AZURE_AI_CHAT_MODEL', 'gpt-4')\n",
    "\n",
    "# Clustering Configuration\n",
    "NUM_CLUSTERS = 5  # Adjust based on your data (used for kmeans and llm methods)\n",
    "CLUSTERING_METHOD = 'llm'  # Options: 'kmeans', 'dbscan', 'llm'\n",
    "# LLM clustering uses the AI model to intelligently group semantic data\n",
    "# This provides better semantic understanding but is slower and uses more API calls\n",
    "\n",
    "# Validate configuration\n",
    "if not AZURE_AI_ENDPOINT or not AZURE_AI_KEY:\n",
    "    print(\"\u26a0\ufe0f  Warning: Azure AI credentials not found in environment variables.\")\n",
    "    print(\"Please set AZURE_AI_ENDPOINT and AZURE_AI_KEY before proceeding.\")\n",
    "else:\n",
    "    print(\"\u2713 Configuration loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Azure AI Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_azure_clients():\n",
    "    \"\"\"\n",
    "    Initialize Azure AI clients for embeddings and chat completions.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple[EmbeddingsClient, ChatCompletionsClient]: Initialized clients\n",
    "    \"\"\"\n",
    "    try:\n",
    "        credential = AzureKeyCredential(AZURE_AI_KEY)\n",
    "        \n",
    "        embeddings_client = EmbeddingsClient(\n",
    "            endpoint=AZURE_AI_ENDPOINT,\n",
    "            credential=credential\n",
    "        )\n",
    "        \n",
    "        chat_client = ChatCompletionsClient(\n",
    "            endpoint=AZURE_AI_ENDPOINT,\n",
    "            credential=credential\n",
    "        )\n",
    "        \n",
    "        print(\"\u2713 Azure AI clients initialized successfully\")\n",
    "        return embeddings_client, chat_client\n",
    "    except Exception as e:\n",
    "        print(f\"\u274c Error initializing Azure AI clients: {str(e)}\")\n",
    "        return None, None\n",
    "\n",
    "# Initialize clients\n",
    "if AZURE_AI_ENDPOINT and AZURE_AI_KEY:\n",
    "    embeddings_client, chat_client = initialize_azure_clients()\n",
    "else:\n",
    "    embeddings_client, chat_client = None, None\n",
    "    print(\"\u26a0\ufe0f  Skipping client initialization due to missing credentials\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_group_data(file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load semantic data from CSV and group by sequence_uuid.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the CSV file with columns: sequence_uuid, semantic_data\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with grouped semantic data\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(f\"\u2713 Loaded {len(df)} rows from {file_path}\")\n",
    "    \n",
    "    # Validate required columns\n",
    "    required_columns = ['sequence_uuid', 'semantic_data']\n",
    "    missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "    if missing_columns:\n",
    "        raise ValueError(f\"Missing required columns: {missing_columns}\")\n",
    "    \n",
    "    # Group by sequence_uuid and combine semantic data\n",
    "    grouped_df = df.groupby('sequence_uuid').agg({\n",
    "        'semantic_data': lambda x: ' '.join(x.astype(str))\n",
    "    }).reset_index()\n",
    "    \n",
    "    grouped_df.rename(columns={'semantic_data': 'combined_semantic_data'}, inplace=True)\n",
    "    \n",
    "    print(f\"\u2713 Grouped into {len(grouped_df)} unique sequence_uuids\")\n",
    "    \n",
    "    return grouped_df\n",
    "\n",
    "# Load sample data\n",
    "data_file = 'sample_data.csv'\n",
    "if os.path.exists(data_file):\n",
    "    df_grouped = load_and_group_data(data_file)\n",
    "    print(\"\\nSample grouped data:\")\n",
    "    print(df_grouped.head())\n",
    "else:\n",
    "    print(f\"\u26a0\ufe0f  Data file '{data_file}' not found. Please provide your data file.\")\n",
    "    df_grouped = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Embeddings using Azure AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(texts: List[str], client: EmbeddingsClient, batch_size: int = 10) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generate embeddings for a list of texts using Azure AI.\n",
    "    \n",
    "    Args:\n",
    "        texts: List of text strings to embed\n",
    "        client: Azure AI EmbeddingsClient\n",
    "        batch_size: Number of texts to process in each batch\n",
    "        \n",
    "    Returns:\n",
    "        NumPy array of embeddings\n",
    "    \"\"\"\n",
    "    all_embeddings = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Generating embeddings\"):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        \n",
    "        try:\n",
    "            response = client.embed(\n",
    "                input=batch,\n",
    "                model=AZURE_AI_MODEL_NAME\n",
    "            )\n",
    "            \n",
    "            batch_embeddings = [item.embedding for item in response.data]\n",
    "            all_embeddings.extend(batch_embeddings)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\u274c Error generating embeddings for batch {i}: {str(e)}\")\n",
    "            # Return zero embeddings for failed batch\n",
    "            all_embeddings.extend([np.zeros(1536) for _ in batch])\n",
    "    \n",
    "    return np.array(all_embeddings)\n",
    "\n",
    "# Generate embeddings\n",
    "if df_grouped is not None and embeddings_client is not None:\n",
    "    texts = df_grouped['combined_semantic_data'].tolist()\n",
    "    embeddings = get_embeddings(texts, embeddings_client)\n",
    "    print(f\"\\n\u2713 Generated embeddings with shape: {embeddings.shape}\")\n",
    "    \n",
    "    # Add embeddings to dataframe\n",
    "    df_grouped['embedding'] = list(embeddings)\n",
    "else:\n",
    "    print(\"\u26a0\ufe0f  Skipping embedding generation\")\n",
    "    embeddings = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_clustering(embeddings: np.ndarray, texts: List[str], method: str = 'kmeans', \n",
    "                       n_clusters: int = 5, chat_client: ChatCompletionsClient = None) -> Tuple[np.ndarray, object]:\n",
    "    \"\"\"\n",
    "    Perform clustering on embeddings.\n",
    "    \n",
    "    Args:\n",
    "        embeddings: NumPy array of embeddings\n",
    "        texts: List of text strings (required for LLM clustering)\n",
    "        method: Clustering method ('kmeans', 'dbscan', or 'llm')\n",
    "        n_clusters: Number of clusters (for kmeans and llm)\n",
    "        chat_client: Azure AI ChatCompletionsClient (required for LLM clustering)\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (cluster labels, clustering model)\n",
    "    \"\"\"\n",
    "    if method == 'llm':\n",
    "        print(\"Using LLM-based clustering...\")\n",
    "        labels, model = perform_llm_clustering(texts, chat_client, n_clusters)\n",
    "        return labels, model\n",
    "    \n",
    "    # Traditional clustering methods\n",
    "    # Standardize embeddings\n",
    "    scaler = StandardScaler()\n",
    "    embeddings_scaled = scaler.fit_transform(embeddings)\n",
    "    \n",
    "    if method == 'kmeans':\n",
    "        model = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "        labels = model.fit_predict(embeddings_scaled)\n",
    "        \n",
    "    elif method == 'dbscan':\n",
    "        model = DBSCAN(eps=0.5, min_samples=2)\n",
    "        labels = model.fit_predict(embeddings_scaled)\n",
    "        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown clustering method: {method}\")\n",
    "    \n",
    "    # Calculate silhouette score\n",
    "    if len(set(labels)) > 1:\n",
    "        score = silhouette_score(embeddings_scaled, labels)\n",
    "        print(f\"\u2713 Clustering completed with {len(set(labels))} clusters\")\n",
    "        print(f\"  Silhouette Score: {score:.3f}\")\n",
    "    else:\n",
    "        print(f\"\u2713 Clustering completed with {len(set(labels))} cluster\")\n",
    "    \n",
    "    return labels, model\n",
    "\n",
    "\n",
    "def perform_llm_clustering(texts: List[str], client: ChatCompletionsClient, n_clusters: int = 5) -> Tuple[np.ndarray, str]:\n",
    "    \"\"\"\n",
    "    Perform clustering using LLM to semantically group texts.\n",
    "    \n",
    "    This method uses the LLM to intelligently group texts based on semantic similarity.\n",
    "    It's more accurate for semantic clustering but slower and uses more API calls.\n",
    "    \n",
    "    Args:\n",
    "        texts: List of text strings to cluster\n",
    "        client: Azure AI ChatCompletionsClient\n",
    "        n_clusters: Desired number of clusters\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (cluster labels array, method description)\n",
    "    \"\"\"\n",
    "    import json as json_lib\n",
    "    \n",
    "    if client is None:\n",
    "        raise ValueError(\"ChatCompletionsClient is required for LLM clustering\")\n",
    "    \n",
    "    # For large datasets, we'll use a hierarchical approach\n",
    "    # First, we'll ask LLM to suggest clusters, then assign items\n",
    "    \n",
    "    print(f\"Step 1: Analyzing {len(texts)} items to identify {n_clusters} semantic clusters...\")\n",
    "    \n",
    "    # Create a sample of texts for initial cluster identification\n",
    "    sample_size = min(20, len(texts))\n",
    "    sample_indices = np.linspace(0, len(texts)-1, sample_size, dtype=int)\n",
    "    sample_texts = [texts[i] for i in sample_indices]\n",
    "    \n",
    "    # Ask LLM to identify cluster themes\n",
    "    sample_formatted = \"\\n\".join([f\"{i+1}. {text[:200]}\" for i, text in enumerate(sample_texts)])\n",
    "    \n",
    "    cluster_identification_prompt = f\"\"\"Analyze the following semantic data samples and identify {n_clusters} distinct thematic clusters.\n",
    "\n",
    "Samples:\n",
    "{sample_formatted}\n",
    "\n",
    "Provide exactly {n_clusters} cluster themes as a JSON array. Each theme should be a concise description (3-6 words).\n",
    "\n",
    "Format your response as valid JSON:\n",
    "{{\"clusters\": [\"Theme 1\", \"Theme 2\", ...]}}\n",
    "\n",
    "Provide ONLY the JSON, no other text.\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = client.complete(\n",
    "            messages=[\n",
    "                SystemMessage(content=\"You are a data analyst expert at identifying semantic patterns and themes in text data. Always respond with valid JSON.\"),\n",
    "                UserMessage(content=cluster_identification_prompt)\n",
    "            ],\n",
    "            model=AZURE_AI_CHAT_MODEL,\n",
    "            temperature=0.3,\n",
    "            max_tokens=500\n",
    "        )\n",
    "        \n",
    "        response_text = response.choices[0].message.content.strip()\n",
    "        # Try to extract JSON from response\n",
    "        if \"```json\" in response_text:\n",
    "            response_text = response_text.split(\"```json\")[1].split(\"```\")[0].strip()\n",
    "        elif \"```\" in response_text:\n",
    "            response_text = response_text.split(\"```\")[1].split(\"```\")[0].strip()\n",
    "        \n",
    "        cluster_themes = json_lib.loads(response_text)[\"clusters\"][:n_clusters]\n",
    "        \n",
    "        # Ensure we have exactly n_clusters\n",
    "        while len(cluster_themes) < n_clusters:\n",
    "            cluster_themes.append(f\"Cluster {len(cluster_themes) + 1}\")\n",
    "        cluster_themes = cluster_themes[:n_clusters]\n",
    "        \n",
    "        print(f\"\u2713 Identified cluster themes: {cluster_themes}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\u26a0\ufe0f  Error identifying clusters with LLM: {str(e)}\")\n",
    "        print(\"  Falling back to generic cluster names\")\n",
    "        cluster_themes = [f\"Cluster {i+1}\" for i in range(n_clusters)]\n",
    "    \n",
    "    # Step 2: Assign each text to a cluster\n",
    "    print(f\"\\nStep 2: Assigning items to clusters...\")\n",
    "    labels = []\n",
    "    \n",
    "    # Process in batches to avoid token limits\n",
    "    batch_size = 5\n",
    "    \n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Assigning clusters\"):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        batch_labels = []\n",
    "        \n",
    "        for text in batch_texts:\n",
    "            # Truncate very long texts\n",
    "            text_truncated = text[:500] if len(text) > 500 else text\n",
    "            \n",
    "            themes_formatted = \"\\n\".join([f\"{j}. {theme}\" for j, theme in enumerate(cluster_themes)])\n",
    "            \n",
    "            assignment_prompt = f\"\"\"Given the following semantic data, assign it to the most appropriate cluster.\n",
    "\n",
    "Semantic data:\n",
    "{text_truncated}\n",
    "\n",
    "Available clusters:\n",
    "{themes_formatted}\n",
    "\n",
    "Respond with ONLY the cluster number (0-{n_clusters-1}), nothing else.\"\"\"\n",
    "\n",
    "            try:\n",
    "                response = client.complete(\n",
    "                    messages=[\n",
    "                        SystemMessage(content=\"You are a data classification expert. Respond only with the cluster number.\"),\n",
    "                        UserMessage(content=assignment_prompt)\n",
    "                    ],\n",
    "                    model=AZURE_AI_CHAT_MODEL,\n",
    "                    temperature=0.1,\n",
    "                    max_tokens=10\n",
    "                )\n",
    "                \n",
    "                cluster_num = response.choices[0].message.content.strip()\n",
    "                # Extract first number found\n",
    "                import re\n",
    "                numbers = re.findall(r'\\d+', cluster_num)\n",
    "                if numbers:\n",
    "                    cluster_id = int(numbers[0])\n",
    "                    # Ensure valid cluster ID\n",
    "                    cluster_id = max(0, min(cluster_id, n_clusters - 1))\n",
    "                else:\n",
    "                    cluster_id = 0\n",
    "                \n",
    "                batch_labels.append(cluster_id)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"\u26a0\ufe0f  Error assigning text to cluster: {str(e)}\")\n",
    "                # Assign to cluster 0 as fallback\n",
    "                batch_labels.append(0)\n",
    "        \n",
    "        labels.extend(batch_labels)\n",
    "    \n",
    "    labels_array = np.array(labels)\n",
    "    \n",
    "    print(f\"\\n\u2713 LLM clustering completed with {len(set(labels))} clusters\")\n",
    "    print(f\"  Cluster distribution: {np.bincount(labels_array)}\")\n",
    "    \n",
    "    # Store cluster themes for later use\n",
    "    llm_model_info = {\n",
    "        'method': 'llm',\n",
    "        'cluster_themes': cluster_themes\n",
    "    }\n",
    "    \n",
    "    return labels_array, llm_model_info\n",
    "\n",
    "\n",
    "# Perform clustering\n",
    "if embeddings is not None:\n",
    "    texts_for_clustering = df_grouped['combined_semantic_data'].tolist()\n",
    "    \n",
    "    cluster_labels, clustering_model = perform_clustering(\n",
    "        embeddings, \n",
    "        texts_for_clustering,\n",
    "        method=CLUSTERING_METHOD, \n",
    "        n_clusters=NUM_CLUSTERS,\n",
    "        chat_client=chat_client\n",
    "    )\n",
    "    \n",
    "    # Add cluster labels to dataframe\n",
    "    df_grouped['cluster'] = cluster_labels\n",
    "    \n",
    "    # If LLM clustering was used, store the themes\n",
    "    if CLUSTERING_METHOD == 'llm' and isinstance(clustering_model, dict):\n",
    "        llm_cluster_themes = clustering_model.get('cluster_themes', {})\n",
    "    else:\n",
    "        llm_cluster_themes = None\n",
    "    \n",
    "    # Display cluster distribution\n",
    "    print(\"\\nCluster distribution:\")\n",
    "    print(df_grouped['cluster'].value_counts().sort_index())\n",
    "else:\n",
    "    print(\"\u26a0\ufe0f  Skipping clustering\")\n",
    "    cluster_labels = None\n",
    "    llm_cluster_themes = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Cluster Titles using LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cluster_title(cluster_texts: List[str], client: ChatCompletionsClient) -> str:\n",
    "    \"\"\"\n",
    "    Generate a descriptive title for a cluster using LLM.\n",
    "    \n",
    "    Args:\n",
    "        cluster_texts: List of texts in the cluster\n",
    "        client: Azure AI ChatCompletionsClient\n",
    "        \n",
    "    Returns:\n",
    "        Generated cluster title\n",
    "    \"\"\"\n",
    "    # Prepare sample texts (limit to avoid token limits)\n",
    "    sample_size = min(10, len(cluster_texts))\n",
    "    sample_texts = cluster_texts[:sample_size]\n",
    "    \n",
    "    # Create prompt\n",
    "    texts_formatted = \"\\n\".join([f\"{i+1}. {text}\" for i, text in enumerate(sample_texts)])\n",
    "    \n",
    "    prompt = f\"\"\"Analyze the following semantic data entries and provide a concise, descriptive title (3-6 words) that captures the main theme or topic:\n",
    "\n",
    "{texts_formatted}\n",
    "\n",
    "Provide only the title, nothing else.\"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = client.complete(\n",
    "            messages=[\n",
    "                SystemMessage(content=\"You are a helpful assistant that analyzes text data and provides concise, descriptive titles.\"),\n",
    "                UserMessage(content=prompt)\n",
    "            ],\n",
    "            model=AZURE_AI_CHAT_MODEL,\n",
    "            temperature=0.3,\n",
    "            max_tokens=50\n",
    "        )\n",
    "        \n",
    "        title = response.choices[0].message.content.strip()\n",
    "        # Remove quotes if present\n",
    "        title = title.strip('\"\\'')\n",
    "        return title\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\u274c Error generating cluster title: {str(e)}\")\n",
    "        return f\"Cluster (Error generating title)\"\n",
    "\n",
    "def generate_all_cluster_titles(df: pd.DataFrame, client: ChatCompletionsClient, \n",
    "                                llm_themes: List[str] = None) -> Dict[int, str]:\n",
    "    \"\"\"\n",
    "    Generate titles for all clusters.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with cluster assignments\n",
    "        client: Azure AI ChatCompletionsClient\n",
    "        llm_themes: Pre-generated themes from LLM clustering (optional)\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mapping cluster ID to title\n",
    "    \"\"\"\n",
    "    cluster_titles = {}\n",
    "    \n",
    "    # If LLM themes are available, use them directly\n",
    "    if llm_themes is not None and len(llm_themes) > 0:\n",
    "        print(\"Using LLM-generated cluster themes...\")\n",
    "        for cluster_id in sorted(df['cluster'].unique()):\n",
    "            if cluster_id < len(llm_themes):\n",
    "                cluster_titles[cluster_id] = llm_themes[cluster_id]\n",
    "                print(f\"Cluster {cluster_id}: {llm_themes[cluster_id]}\")\n",
    "            else:\n",
    "                cluster_titles[cluster_id] = f\"Cluster {cluster_id}\"\n",
    "        return cluster_titles\n",
    "    \n",
    "    # Otherwise, generate titles using LLM\n",
    "    print(\"Generating cluster titles using LLM analysis...\")\n",
    "    for cluster_id in sorted(df['cluster'].unique()):\n",
    "        cluster_data = df[df['cluster'] == cluster_id]\n",
    "        cluster_texts = cluster_data['combined_semantic_data'].tolist()\n",
    "        \n",
    "        print(f\"Generating title for Cluster {cluster_id} ({len(cluster_texts)} items)...\")\n",
    "        title = generate_cluster_title(cluster_texts, client)\n",
    "        cluster_titles[cluster_id] = title\n",
    "        print(f\"  \u2192 {title}\")\n",
    "    \n",
    "    return cluster_titles\n",
    "\n",
    "# Generate cluster titles\n",
    "if df_grouped is not None and cluster_labels is not None and chat_client is not None:\n",
    "    print(\"\\nGenerating cluster titles...\")\n",
    "    cluster_titles = generate_all_cluster_titles(df_grouped, chat_client, llm_cluster_themes)\n",
    "    \n",
    "    # Add titles to dataframe\n",
    "    df_grouped['cluster_title'] = df_grouped['cluster'].map(cluster_titles)\n",
    "    \n",
    "    print(\"\\n\u2713 Cluster titles generated successfully\")\n",
    "else:\n",
    "    print(\"\u26a0\ufe0f  Skipping cluster title generation\")\n",
    "    cluster_titles = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_clusters(embeddings: np.ndarray, labels: np.ndarray, titles: Dict[int, str] = None):\n",
    "    \"\"\"\n",
    "    Visualize clusters using PCA dimensionality reduction.\n",
    "    \n",
    "    Args:\n",
    "        embeddings: NumPy array of embeddings\n",
    "        labels: Cluster labels\n",
    "        titles: Dictionary mapping cluster ID to title\n",
    "    \"\"\"\n",
    "    # Reduce dimensions to 2D using PCA\n",
    "    pca = PCA(n_components=2, random_state=42)\n",
    "    embeddings_2d = pca.fit_transform(embeddings)\n",
    "    \n",
    "    # Create plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Get unique clusters\n",
    "    unique_labels = sorted(set(labels))\n",
    "    colors = sns.color_palette('husl', len(unique_labels))\n",
    "    \n",
    "    # Plot each cluster\n",
    "    for i, cluster_id in enumerate(unique_labels):\n",
    "        mask = labels == cluster_id\n",
    "        label = titles[cluster_id] if titles else f\"Cluster {cluster_id}\"\n",
    "        \n",
    "        plt.scatter(\n",
    "            embeddings_2d[mask, 0],\n",
    "            embeddings_2d[mask, 1],\n",
    "            c=[colors[i]],\n",
    "            label=label,\n",
    "            alpha=0.6,\n",
    "            s=100,\n",
    "            edgecolors='black',\n",
    "            linewidth=0.5\n",
    "        )\n",
    "    \n",
    "    plt.xlabel('First Principal Component', fontsize=12)\n",
    "    plt.ylabel('Second Principal Component', fontsize=12)\n",
    "    plt.title('Semantic Data Clusters (PCA Visualization)', fontsize=14, fontweight='bold')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    variance_explained = pca.explained_variance_ratio_\n",
    "    print(f\"\\nPCA Variance Explained:\")\n",
    "    print(f\"  PC1: {variance_explained[0]:.2%}\")\n",
    "    print(f\"  PC2: {variance_explained[1]:.2%}\")\n",
    "    print(f\"  Total: {sum(variance_explained):.2%}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Visualize clusters\n",
    "if embeddings is not None and cluster_labels is not None:\n",
    "    visualize_clusters(embeddings, cluster_labels, cluster_titles)\n",
    "else:\n",
    "    print(\"\u26a0\ufe0f  Skipping visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Cluster Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_cluster_summary(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Display a summary of all clusters with sample data.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with cluster assignments and titles\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"CLUSTER ANALYSIS SUMMARY\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for cluster_id in sorted(df['cluster'].unique()):\n",
    "        cluster_data = df[df['cluster'] == cluster_id]\n",
    "        title = cluster_data.iloc[0]['cluster_title'] if 'cluster_title' in cluster_data.columns else f\"Cluster {cluster_id}\"\n",
    "        \n",
    "        print(f\"\\n{'\u2500' * 80}\")\n",
    "        print(f\"CLUSTER {cluster_id}: {title}\")\n",
    "        print(f\"{'\u2500' * 80}\")\n",
    "        print(f\"Size: {len(cluster_data)} items\")\n",
    "        print(f\"\\nSample entries:\")\n",
    "        \n",
    "        # Show up to 3 sample entries\n",
    "        for idx, (_, row) in enumerate(cluster_data.head(3).iterrows(), 1):\n",
    "            text = row['combined_semantic_data']\n",
    "            # Truncate long texts\n",
    "            if len(text) > 150:\n",
    "                text = text[:150] + \"...\"\n",
    "            print(f\"  {idx}. [{row['sequence_uuid']}] {text}\")\n",
    "        \n",
    "        if len(cluster_data) > 3:\n",
    "            print(f\"  ... and {len(cluster_data) - 3} more items\")\n",
    "    \n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"Total: {len(df)} items across {len(df['cluster'].unique())} clusters\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "\n",
    "# Display summary\n",
    "if df_grouped is not None and cluster_labels is not None:\n",
    "    display_cluster_summary(df_grouped)\n",
    "else:\n",
    "    print(\"\u26a0\ufe0f  No cluster data to display\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_results(df: pd.DataFrame, output_file: str = 'clustered_results.csv'):\n",
    "    \"\"\"\n",
    "    Export clustering results to CSV.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with cluster assignments\n",
    "        output_file: Output file path\n",
    "    \"\"\"\n",
    "    # Prepare export dataframe (exclude embedding column)\n",
    "    export_df = df.copy()\n",
    "    if 'embedding' in export_df.columns:\n",
    "        export_df = export_df.drop(columns=['embedding'])\n",
    "    \n",
    "    # Save to CSV\n",
    "    export_df.to_csv(output_file, index=False)\n",
    "    print(f\"\u2713 Results exported to {output_file}\")\n",
    "    \n",
    "    return export_df\n",
    "\n",
    "# Export results\n",
    "if df_grouped is not None and cluster_labels is not None:\n",
    "    results_df = export_results(df_grouped)\n",
    "    print(\"\\nExported columns:\", results_df.columns.tolist())\n",
    "else:\n",
    "    print(\"\u26a0\ufe0f  No results to export\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage Instructions\n",
    "\n",
    "### Step 1: Set up Environment Variables\n",
    "Create a `.env` file in the same directory with:\n",
    "```\n",
    "AZURE_AI_ENDPOINT=https://your-endpoint.cognitiveservices.azure.com/\n",
    "AZURE_AI_KEY=your-api-key-here\n",
    "AZURE_AI_MODEL_NAME=text-embedding-ada-002\n",
    "AZURE_AI_CHAT_MODEL=gpt-4\n",
    "```\n",
    "\n",
    "### Step 2: Prepare Your Data\n",
    "Create a CSV file with the following structure:\n",
    "- `sequence_uuid`: Unique identifier for grouping related semantic data\n",
    "- `semantic_data`: The text content to be analyzed\n",
    "\n",
    "Example:\n",
    "```csv\n",
    "sequence_uuid,semantic_data\n",
    "seq_001,First line of semantic data\n",
    "seq_001,Second line related to seq_001\n",
    "seq_002,Different semantic data\n",
    "```\n",
    "\n",
    "### Step 3: Configure Clustering\n",
    "In the Configuration cell, adjust:\n",
    "- `NUM_CLUSTERS`: Number of clusters (for K-means and LLM methods)\n",
    "- `CLUSTERING_METHOD`: Choose from:\n",
    "  - **'llm'** (Recommended): Uses LLM to intelligently group data based on semantic meaning\n",
    "    - More accurate semantic understanding\n",
    "    - Automatically identifies cluster themes\n",
    "    - Slower and uses more API calls\n",
    "  - **'kmeans'**: Traditional K-means clustering\n",
    "    - Fast and efficient\n",
    "    - Good for well-separated clusters\n",
    "    - Requires knowing the number of clusters\n",
    "  - **'dbscan'**: Density-based clustering\n",
    "    - Automatically detects number of clusters\n",
    "    - Good for arbitrary-shaped clusters\n",
    "    - May create noise/outlier category\n",
    "\n",
    "### Step 4: Run the Notebook\n",
    "Execute all cells in order. The notebook will:\n",
    "1. Load and group your data by `sequence_uuid`\n",
    "2. Generate embeddings using Azure AI\n",
    "3. Perform clustering (using your chosen method)\n",
    "4. Generate descriptive titles for each cluster\n",
    "5. Visualize the results\n",
    "6. Export the clustered data\n",
    "\n",
    "### Clustering Method Comparison\n",
    "\n",
    "| Method | Speed | Accuracy | API Calls | Best For |\n",
    "|--------|-------|----------|-----------|----------|\n",
    "| LLM | Slow | High | Many | Semantic understanding, complex themes |\n",
    "| K-means | Fast | Medium | Few | Large datasets, known cluster count |\n",
    "| DBSCAN | Medium | Medium | Few | Automatic cluster detection |\n",
    "\n",
    "### Customization\n",
    "- **LLM clustering parameters**: \n",
    "  - Adjust `NUM_CLUSTERS` to control how many semantic groups to identify\n",
    "  - The LLM will analyze your data and create meaningful cluster themes\n",
    "- **Traditional clustering**: \n",
    "  - Modify `CLUSTERING_METHOD` to 'kmeans' or 'dbscan'\n",
    "  - Adjust `NUM_CLUSTERS` for K-means\n",
    "- **Batch processing**: \n",
    "  - Modify `batch_size` in `get_embeddings()` if you hit rate limits\n",
    "  - LLM clustering processes texts in batches of 5 by default\n",
    "- **Visualization**: \n",
    "  - Customize colors and plot settings in `visualize_clusters()`\n",
    "\n",
    "### Tips for LLM Clustering\n",
    "- Works best with 3-10 clusters for optimal semantic grouping\n",
    "- Larger datasets will take longer due to individual item classification\n",
    "- Consider using a sample of your data first to test cluster quality\n",
    "- The LLM identifies themes first, then assigns each item to the best-fit cluster"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}