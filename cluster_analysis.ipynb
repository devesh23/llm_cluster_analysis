{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM-Based Semantic Cluster Analysis\n",
    "\n",
    "This notebook performs cluster analysis on semantic data using Azure AI Foundry APIs.\n",
    "\n",
    "## Features\n",
    "- Handles multi-line semantic data grouped by `sequence_uuid`\n",
    "- Uses Azure AI embeddings for semantic understanding\n",
    "- Automatically generates cluster titles using LLM\n",
    "- Visualizes clusters and their characteristics\n",
    "\n",
    "## Prerequisites\n",
    "Before running this notebook, ensure you have:\n",
    "1. Azure AI Foundry API credentials\n",
    "2. Set environment variables:\n",
    "   - `AZURE_AI_ENDPOINT`: Your Azure AI endpoint URL\n",
    "   - `AZURE_AI_KEY`: Your Azure AI API key\n",
    "   - `AZURE_AI_MODEL_NAME`: The model name for embeddings (e.g., 'text-embedding-ada-002')\n",
    "   - `AZURE_AI_CHAT_MODEL`: The chat model for cluster naming (e.g., 'gpt-4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Azure AI\n",
    "from azure.ai.inference import ChatCompletionsClient, EmbeddingsClient\n",
    "from azure.ai.inference.models import SystemMessage, UserMessage\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "# Clustering and ML\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Utilities\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"✓ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Azure AI Configuration\n",
    "AZURE_AI_ENDPOINT = os.getenv('AZURE_AI_ENDPOINT', '')\n",
    "AZURE_AI_KEY = os.getenv('AZURE_AI_KEY', '')\n",
    "AZURE_AI_MODEL_NAME = os.getenv('AZURE_AI_MODEL_NAME', 'text-embedding-ada-002')\n",
    "AZURE_AI_CHAT_MODEL = os.getenv('AZURE_AI_CHAT_MODEL', 'gpt-4')\n",
    "\n",
    "# Clustering Configuration\n",
    "NUM_CLUSTERS = 5  # Adjust based on your data\n",
    "CLUSTERING_METHOD = 'kmeans'  # Options: 'kmeans', 'dbscan'\n",
    "\n",
    "# Validate configuration\n",
    "if not AZURE_AI_ENDPOINT or not AZURE_AI_KEY:\n",
    "    print(\"⚠️  Warning: Azure AI credentials not found in environment variables.\")\n",
    "    print(\"Please set AZURE_AI_ENDPOINT and AZURE_AI_KEY before proceeding.\")\n",
    "else:\n",
    "    print(\"✓ Configuration loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Azure AI Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_azure_clients():\n",
    "    \"\"\"\n",
    "    Initialize Azure AI clients for embeddings and chat completions.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple[EmbeddingsClient, ChatCompletionsClient]: Initialized clients\n",
    "    \"\"\"\n",
    "    try:\n",
    "        credential = AzureKeyCredential(AZURE_AI_KEY)\n",
    "        \n",
    "        embeddings_client = EmbeddingsClient(\n",
    "            endpoint=AZURE_AI_ENDPOINT,\n",
    "            credential=credential\n",
    "        )\n",
    "        \n",
    "        chat_client = ChatCompletionsClient(\n",
    "            endpoint=AZURE_AI_ENDPOINT,\n",
    "            credential=credential\n",
    "        )\n",
    "        \n",
    "        print(\"✓ Azure AI clients initialized successfully\")\n",
    "        return embeddings_client, chat_client\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error initializing Azure AI clients: {str(e)}\")\n",
    "        return None, None\n",
    "\n",
    "# Initialize clients\n",
    "if AZURE_AI_ENDPOINT and AZURE_AI_KEY:\n",
    "    embeddings_client, chat_client = initialize_azure_clients()\n",
    "else:\n",
    "    embeddings_client, chat_client = None, None\n",
    "    print(\"⚠️  Skipping client initialization due to missing credentials\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_group_data(file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load semantic data from CSV and group by sequence_uuid.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the CSV file with columns: sequence_uuid, semantic_data\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with grouped semantic data\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(f\"✓ Loaded {len(df)} rows from {file_path}\")\n",
    "    \n",
    "    # Validate required columns\n",
    "    required_columns = ['sequence_uuid', 'semantic_data']\n",
    "    missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "    if missing_columns:\n",
    "        raise ValueError(f\"Missing required columns: {missing_columns}\")\n",
    "    \n",
    "    # Group by sequence_uuid and combine semantic data\n",
    "    grouped_df = df.groupby('sequence_uuid').agg({\n",
    "        'semantic_data': lambda x: ' '.join(x.astype(str))\n",
    "    }).reset_index()\n",
    "    \n",
    "    grouped_df.rename(columns={'semantic_data': 'combined_semantic_data'}, inplace=True)\n",
    "    \n",
    "    print(f\"✓ Grouped into {len(grouped_df)} unique sequence_uuids\")\n",
    "    \n",
    "    return grouped_df\n",
    "\n",
    "# Load sample data\n",
    "data_file = 'sample_data.csv'\n",
    "if os.path.exists(data_file):\n",
    "    df_grouped = load_and_group_data(data_file)\n",
    "    print(\"\\nSample grouped data:\")\n",
    "    print(df_grouped.head())\n",
    "else:\n",
    "    print(f\"⚠️  Data file '{data_file}' not found. Please provide your data file.\")\n",
    "    df_grouped = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Embeddings using Azure AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(texts: List[str], client: EmbeddingsClient, batch_size: int = 10) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generate embeddings for a list of texts using Azure AI.\n",
    "    \n",
    "    Args:\n",
    "        texts: List of text strings to embed\n",
    "        client: Azure AI EmbeddingsClient\n",
    "        batch_size: Number of texts to process in each batch\n",
    "        \n",
    "    Returns:\n",
    "        NumPy array of embeddings\n",
    "    \"\"\"\n",
    "    all_embeddings = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Generating embeddings\"):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        \n",
    "        try:\n",
    "            response = client.embed(\n",
    "                input=batch,\n",
    "                model=AZURE_AI_MODEL_NAME\n",
    "            )\n",
    "            \n",
    "            batch_embeddings = [item.embedding for item in response.data]\n",
    "            all_embeddings.extend(batch_embeddings)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error generating embeddings for batch {i}: {str(e)}\")\n",
    "            # Return zero embeddings for failed batch\n",
    "            all_embeddings.extend([np.zeros(1536) for _ in batch])\n",
    "    \n",
    "    return np.array(all_embeddings)\n",
    "\n",
    "# Generate embeddings\n",
    "if df_grouped is not None and embeddings_client is not None:\n",
    "    texts = df_grouped['combined_semantic_data'].tolist()\n",
    "    embeddings = get_embeddings(texts, embeddings_client)\n",
    "    print(f\"\\n✓ Generated embeddings with shape: {embeddings.shape}\")\n",
    "    \n",
    "    # Add embeddings to dataframe\n",
    "    df_grouped['embedding'] = list(embeddings)\n",
    "else:\n",
    "    print(\"⚠️  Skipping embedding generation\")\n",
    "    embeddings = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_clustering(embeddings: np.ndarray, method: str = 'kmeans', n_clusters: int = 5) -> Tuple[np.ndarray, object]:\n",
    "    \"\"\"\n",
    "    Perform clustering on embeddings.\n",
    "    \n",
    "    Args:\n",
    "        embeddings: NumPy array of embeddings\n",
    "        method: Clustering method ('kmeans' or 'dbscan')\n",
    "        n_clusters: Number of clusters (for kmeans)\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (cluster labels, clustering model)\n",
    "    \"\"\"\n",
    "    # Standardize embeddings\n",
    "    scaler = StandardScaler()\n",
    "    embeddings_scaled = scaler.fit_transform(embeddings)\n",
    "    \n",
    "    if method == 'kmeans':\n",
    "        model = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "        labels = model.fit_predict(embeddings_scaled)\n",
    "        \n",
    "    elif method == 'dbscan':\n",
    "        model = DBSCAN(eps=0.5, min_samples=2)\n",
    "        labels = model.fit_predict(embeddings_scaled)\n",
    "        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown clustering method: {method}\")\n",
    "    \n",
    "    # Calculate silhouette score\n",
    "    if len(set(labels)) > 1:\n",
    "        score = silhouette_score(embeddings_scaled, labels)\n",
    "        print(f\"✓ Clustering completed with {len(set(labels))} clusters\")\n",
    "        print(f\"  Silhouette Score: {score:.3f}\")\n",
    "    else:\n",
    "        print(f\"✓ Clustering completed with {len(set(labels))} cluster\")\n",
    "    \n",
    "    return labels, model\n",
    "\n",
    "# Perform clustering\n",
    "if embeddings is not None:\n",
    "    cluster_labels, clustering_model = perform_clustering(\n",
    "        embeddings, \n",
    "        method=CLUSTERING_METHOD, \n",
    "        n_clusters=NUM_CLUSTERS\n",
    "    )\n",
    "    \n",
    "    # Add cluster labels to dataframe\n",
    "    df_grouped['cluster'] = cluster_labels\n",
    "    \n",
    "    # Display cluster distribution\n",
    "    print(\"\\nCluster distribution:\")\n",
    "    print(df_grouped['cluster'].value_counts().sort_index())\n",
    "else:\n",
    "    print(\"⚠️  Skipping clustering\")\n",
    "    cluster_labels = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Cluster Titles using LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cluster_title(cluster_texts: List[str], client: ChatCompletionsClient) -> str:\n",
    "    \"\"\"\n",
    "    Generate a descriptive title for a cluster using LLM.\n",
    "    \n",
    "    Args:\n",
    "        cluster_texts: List of texts in the cluster\n",
    "        client: Azure AI ChatCompletionsClient\n",
    "        \n",
    "    Returns:\n",
    "        Generated cluster title\n",
    "    \"\"\"\n",
    "    # Prepare sample texts (limit to avoid token limits)\n",
    "    sample_size = min(10, len(cluster_texts))\n",
    "    sample_texts = cluster_texts[:sample_size]\n",
    "    \n",
    "    # Create prompt\n",
    "    texts_formatted = \"\\n\".join([f\"{i+1}. {text}\" for i, text in enumerate(sample_texts)])\n",
    "    \n",
    "    prompt = f\"\"\"Analyze the following semantic data entries and provide a concise, descriptive title (3-6 words) that captures the main theme or topic:\n",
    "\n",
    "{texts_formatted}\n",
    "\n",
    "Provide only the title, nothing else.\"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = client.complete(\n",
    "            messages=[\n",
    "                SystemMessage(content=\"You are a helpful assistant that analyzes text data and provides concise, descriptive titles.\"),\n",
    "                UserMessage(content=prompt)\n",
    "            ],\n",
    "            model=AZURE_AI_CHAT_MODEL,\n",
    "            temperature=0.3,\n",
    "            max_tokens=50\n",
    "        )\n",
    "        \n",
    "        title = response.choices[0].message.content.strip()\n",
    "        # Remove quotes if present\n",
    "        title = title.strip('\"\\'')\n",
    "        return title\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error generating cluster title: {str(e)}\")\n",
    "        return f\"Cluster (Error generating title)\"\n",
    "\n",
    "def generate_all_cluster_titles(df: pd.DataFrame, client: ChatCompletionsClient) -> Dict[int, str]:\n",
    "    \"\"\"\n",
    "    Generate titles for all clusters.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with cluster assignments\n",
    "        client: Azure AI ChatCompletionsClient\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mapping cluster ID to title\n",
    "    \"\"\"\n",
    "    cluster_titles = {}\n",
    "    \n",
    "    for cluster_id in sorted(df['cluster'].unique()):\n",
    "        cluster_data = df[df['cluster'] == cluster_id]\n",
    "        cluster_texts = cluster_data['combined_semantic_data'].tolist()\n",
    "        \n",
    "        print(f\"Generating title for Cluster {cluster_id} ({len(cluster_texts)} items)...\")\n",
    "        title = generate_cluster_title(cluster_texts, client)\n",
    "        cluster_titles[cluster_id] = title\n",
    "        print(f\"  → {title}\")\n",
    "    \n",
    "    return cluster_titles\n",
    "\n",
    "# Generate cluster titles\n",
    "if df_grouped is not None and cluster_labels is not None and chat_client is not None:\n",
    "    print(\"\\nGenerating cluster titles...\")\n",
    "    cluster_titles = generate_all_cluster_titles(df_grouped, chat_client)\n",
    "    \n",
    "    # Add titles to dataframe\n",
    "    df_grouped['cluster_title'] = df_grouped['cluster'].map(cluster_titles)\n",
    "    \n",
    "    print(\"\\n✓ Cluster titles generated successfully\")\n",
    "else:\n",
    "    print(\"⚠️  Skipping cluster title generation\")\n",
    "    cluster_titles = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_clusters(embeddings: np.ndarray, labels: np.ndarray, titles: Dict[int, str] = None):\n",
    "    \"\"\"\n",
    "    Visualize clusters using PCA dimensionality reduction.\n",
    "    \n",
    "    Args:\n",
    "        embeddings: NumPy array of embeddings\n",
    "        labels: Cluster labels\n",
    "        titles: Dictionary mapping cluster ID to title\n",
    "    \"\"\"\n",
    "    # Reduce dimensions to 2D using PCA\n",
    "    pca = PCA(n_components=2, random_state=42)\n",
    "    embeddings_2d = pca.fit_transform(embeddings)\n",
    "    \n",
    "    # Create plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Get unique clusters\n",
    "    unique_labels = sorted(set(labels))\n",
    "    colors = sns.color_palette('husl', len(unique_labels))\n",
    "    \n",
    "    # Plot each cluster\n",
    "    for i, cluster_id in enumerate(unique_labels):\n",
    "        mask = labels == cluster_id\n",
    "        label = titles[cluster_id] if titles else f\"Cluster {cluster_id}\"\n",
    "        \n",
    "        plt.scatter(\n",
    "            embeddings_2d[mask, 0],\n",
    "            embeddings_2d[mask, 1],\n",
    "            c=[colors[i]],\n",
    "            label=label,\n",
    "            alpha=0.6,\n",
    "            s=100,\n",
    "            edgecolors='black',\n",
    "            linewidth=0.5\n",
    "        )\n",
    "    \n",
    "    plt.xlabel('First Principal Component', fontsize=12)\n",
    "    plt.ylabel('Second Principal Component', fontsize=12)\n",
    "    plt.title('Semantic Data Clusters (PCA Visualization)', fontsize=14, fontweight='bold')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    variance_explained = pca.explained_variance_ratio_\n",
    "    print(f\"\\nPCA Variance Explained:\")\n",
    "    print(f\"  PC1: {variance_explained[0]:.2%}\")\n",
    "    print(f\"  PC2: {variance_explained[1]:.2%}\")\n",
    "    print(f\"  Total: {sum(variance_explained):.2%}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Visualize clusters\n",
    "if embeddings is not None and cluster_labels is not None:\n",
    "    visualize_clusters(embeddings, cluster_labels, cluster_titles)\n",
    "else:\n",
    "    print(\"⚠️  Skipping visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Cluster Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_cluster_summary(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Display a summary of all clusters with sample data.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with cluster assignments and titles\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"CLUSTER ANALYSIS SUMMARY\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for cluster_id in sorted(df['cluster'].unique()):\n",
    "        cluster_data = df[df['cluster'] == cluster_id]\n",
    "        title = cluster_data.iloc[0]['cluster_title'] if 'cluster_title' in cluster_data.columns else f\"Cluster {cluster_id}\"\n",
    "        \n",
    "        print(f\"\\n{'─' * 80}\")\n",
    "        print(f\"CLUSTER {cluster_id}: {title}\")\n",
    "        print(f\"{'─' * 80}\")\n",
    "        print(f\"Size: {len(cluster_data)} items\")\n",
    "        print(f\"\\nSample entries:\")\n",
    "        \n",
    "        # Show up to 3 sample entries\n",
    "        for idx, (_, row) in enumerate(cluster_data.head(3).iterrows(), 1):\n",
    "            text = row['combined_semantic_data']\n",
    "            # Truncate long texts\n",
    "            if len(text) > 150:\n",
    "                text = text[:150] + \"...\"\n",
    "            print(f\"  {idx}. [{row['sequence_uuid']}] {text}\")\n",
    "        \n",
    "        if len(cluster_data) > 3:\n",
    "            print(f\"  ... and {len(cluster_data) - 3} more items\")\n",
    "    \n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"Total: {len(df)} items across {len(df['cluster'].unique())} clusters\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "\n",
    "# Display summary\n",
    "if df_grouped is not None and cluster_labels is not None:\n",
    "    display_cluster_summary(df_grouped)\n",
    "else:\n",
    "    print(\"⚠️  No cluster data to display\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_results(df: pd.DataFrame, output_file: str = 'clustered_results.csv'):\n",
    "    \"\"\"\n",
    "    Export clustering results to CSV.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with cluster assignments\n",
    "        output_file: Output file path\n",
    "    \"\"\"\n",
    "    # Prepare export dataframe (exclude embedding column)\n",
    "    export_df = df.copy()\n",
    "    if 'embedding' in export_df.columns:\n",
    "        export_df = export_df.drop(columns=['embedding'])\n",
    "    \n",
    "    # Save to CSV\n",
    "    export_df.to_csv(output_file, index=False)\n",
    "    print(f\"✓ Results exported to {output_file}\")\n",
    "    \n",
    "    return export_df\n",
    "\n",
    "# Export results\n",
    "if df_grouped is not None and cluster_labels is not None:\n",
    "    results_df = export_results(df_grouped)\n",
    "    print(\"\\nExported columns:\", results_df.columns.tolist())\n",
    "else:\n",
    "    print(\"⚠️  No results to export\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage Instructions\n",
    "\n",
    "### Step 1: Set up Environment Variables\n",
    "Create a `.env` file in the same directory with:\n",
    "```\n",
    "AZURE_AI_ENDPOINT=https://your-endpoint.cognitiveservices.azure.com/\n",
    "AZURE_AI_KEY=your-api-key-here\n",
    "AZURE_AI_MODEL_NAME=text-embedding-ada-002\n",
    "AZURE_AI_CHAT_MODEL=gpt-4\n",
    "```\n",
    "\n",
    "### Step 2: Prepare Your Data\n",
    "Create a CSV file with the following structure:\n",
    "- `sequence_uuid`: Unique identifier for grouping related semantic data\n",
    "- `semantic_data`: The text content to be analyzed\n",
    "\n",
    "Example:\n",
    "```csv\n",
    "sequence_uuid,semantic_data\n",
    "seq_001,First line of semantic data\n",
    "seq_001,Second line related to seq_001\n",
    "seq_002,Different semantic data\n",
    "```\n",
    "\n",
    "### Step 3: Configure Clustering\n",
    "In the Configuration cell, adjust:\n",
    "- `NUM_CLUSTERS`: Number of clusters (for K-means)\n",
    "- `CLUSTERING_METHOD`: 'kmeans' or 'dbscan'\n",
    "\n",
    "### Step 4: Run the Notebook\n",
    "Execute all cells in order. The notebook will:\n",
    "1. Load and group your data by `sequence_uuid`\n",
    "2. Generate embeddings using Azure AI\n",
    "3. Perform clustering\n",
    "4. Generate descriptive titles for each cluster\n",
    "5. Visualize the results\n",
    "6. Export the clustered data\n",
    "\n",
    "### Customization\n",
    "- **Different clustering algorithms**: Modify `CLUSTERING_METHOD` to 'dbscan' for density-based clustering\n",
    "- **Custom number of clusters**: Adjust `NUM_CLUSTERS` based on your data\n",
    "- **Batch size**: Modify `batch_size` in `get_embeddings()` if you hit rate limits\n",
    "- **Visualization**: Customize colors and plot settings in `visualize_clusters()`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
